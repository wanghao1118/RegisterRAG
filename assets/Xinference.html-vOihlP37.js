import{_ as e}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as i,o as s,b as n}from"./app-GC2pG_6p.js";const a={},t=n(`<p>Xorbits Inference (Xinference) is an open-source platform to streamline the operation and integration of a wide array of AI models. For more information, see the <a href="https://inference.readthedocs.io/en/latest/" target="_blank" rel="noopener noreferrer">Xinference Documentation</a>.</p><h2 id="basic-configuration" tabindex="-1"><a class="header-anchor" href="#basic-configuration"><span>Basic Configuration</span></a></h2><p>Below is the configuration for the Xinference service in Python:</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">class</span><span style="--shiki-light:#C18401;--shiki-dark:#E5C07B;"> XinferenceConfig</span><span style="--shiki-light:#C18401;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#C18401;--shiki-dark:#E5C07B;">BaseModel</span><span style="--shiki-light:#C18401;--shiki-dark:#ABB2BF;">)</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">:</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    xinference_model_engine: </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">str</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;Transformers&quot;</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    xinference_model_format: </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">str</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;pytorch&quot;</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    xinference_ngpu: </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">str</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;auto&quot;</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    xinference_model_size: </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">str</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    xinference_mdoel_quantization: </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">str</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><code>xinference_model_engine</code>: The engine used for the model.</li><li><code>xinference_model_format</code>: The format of the model.</li><li><code>xinference_ngpu</code>: The number of GPUs used for the model.</li><li><code>xinference_model_size</code>: The size of the model.</li><li><code>xinference_mdoel_quantization</code>: The quantization of the model.</li></ul><h2 id="example-configuration" tabindex="-1"><a class="header-anchor" href="#example-configuration"><span>Example Configuration</span></a></h2><div class="language-json line-numbers-mode" data-highlighter="shiki" data-ext="json" data-title="json" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">{</span></span>
<span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">  &quot;generation&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: {</span></span>
<span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">    &quot;generation_type&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;remote&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">    &quot;generation_model_name_or_path&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;xinference/qwen2-instruct&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">    &quot;generation_remote_url&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;http://localhost:9997&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">    &quot;generation_xinference_config&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: {</span></span>
<span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">      &quot;xinference_model_size&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;1_5&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">      &quot;xinference_mdoel_quantization&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;4-bit&quot;</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    }</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">  }</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">}</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>Note</strong>: The model name is set to <code>xinference/qwen2-instruct</code>. The <code>xinferece</code> prefix is used to indicate that the model is hosted on the Xinference service, and <code>qwen2-instruct</code> is the name of the model. A <code>/</code> is used to separate the service name and the model name.</p>`,8),r=[t];function l(o,h){return s(),i("div",null,r)}const c=e(a,[["render",l],["__file","Xinference.html.vue"]]),k=JSON.parse('{"path":"/Guide/Generation/Xinference.html","title":"Xinference","lang":"en-US","frontmatter":{"lang":"en-US","title":"Xinference","description":"How to configure the Xinference service in Register RAG","head":[["link",{"rel":"alternate","hreflang":"zh-cn","href":"https://github.com/Charon-ops/RegisterRAG/RegisterRAG/zh/Guide/Generation/Xinference.html"}],["meta",{"property":"og:url","content":"https://github.com/Charon-ops/RegisterRAG/RegisterRAG/Guide/Generation/Xinference.html"}],["meta",{"property":"og:site_name","content":"Register RAG"}],["meta",{"property":"og:title","content":"Xinference"}],["meta",{"property":"og:description","content":"How to configure the Xinference service in Register RAG"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:locale:alternate","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2024-08-17T08:42:28.000Z"}],["meta",{"property":"article:author","content":"JLULLM"}],["meta",{"property":"article:modified_time","content":"2024-08-17T08:42:28.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Xinference\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2024-08-17T08:42:28.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"JLULLM\\",\\"email\\":\\"jlullm@163.com\\"}]}"]]},"headers":[{"level":2,"title":"Basic Configuration","slug":"basic-configuration","link":"#basic-configuration","children":[]},{"level":2,"title":"Example Configuration","slug":"example-configuration","link":"#example-configuration","children":[]}],"git":{"createdTime":1723884148000,"updatedTime":1723884148000,"contributors":[{"name":"yumuzhihan","email":"1573252900@qq.com","commits":1}]},"readingTime":{"minutes":0.59,"words":176},"filePathRelative":"Guide/Generation/Xinference.md","localizedDate":"August 17, 2024","excerpt":"<p>Xorbits Inference (Xinference) is an open-source platform to streamline the operation and integration of a wide array of AI models. For more information, see the <a href=\\"https://inference.readthedocs.io/en/latest/\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">Xinference Documentation</a>.</p>\\n<h2>Basic Configuration</h2>"}');export{c as comp,k as data};
